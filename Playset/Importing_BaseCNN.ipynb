{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Importing_BaseCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BybWfRdW47gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0b637a3d-04da-4c25-9565-615e955f4407"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import timeit\n",
        "import numpy as np\n",
        "np.random.seed(1234)  # for reproducibility\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
        "#from keras.regularizers import WeightRegularizer, l1, l2, l1l2\n",
        "from keras.regularizers import Regularizer, l1, l2, l1_l2\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.utils import np_utils\n",
        "from sklearn.svm import SVC\n",
        "import six.moves.cPickle as pickle\n",
        "from sklearn import grid_search\n",
        "from sklearn import metrics\n",
        "\n",
        "import scipy.io as sio  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YM9xX-k947gg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(patch_i,data_i):\n",
        "    #patch_i is an index for which patch in the current brain sample, so it's 0 to 26\n",
        "    #data_i is an index for which brain sample, so it's 0 to num_brains, in the for loop, it is passed in as the index of the current iteration of the loop\n",
        "    data_i = data_i+1 #incrementing data_i by 1 each time, so i think that it goes from 1 to num_brains+1 entries when pulling patch data\n",
        "    dataSetpath = '/media/disk_e/PET_PatchDATA' #specified path to the location of the PET scan patch data, WE WILL NEED TO CHANGE THIS!\n",
        "    filename = 'AffineADNC64_Patch'+str(patch_i)+'_PET_'+str(data_i)+'.mat' #file has to be a .mat file, and, for them\n",
        "                                                                            #it's in the format AffineADNC64_Patch(patch id)_PET_(data id).mat\n",
        "    full_filename= os.path.join(dataSetpath,filename) #creates the full conjoined path of the dataSet's path and the file's name\n",
        "    dataMat = sio.loadmat(full_filename) #loading the data(.mat) file\n",
        "    AD_name = 'PatchAD' #column name for the alzheimer's detection data\n",
        "    AD_data = dataMat[AD_name] #getting the alzheimer's detection patch from the data file\n",
        "    num_AD =  AD_data.shape[0]\n",
        "    NC_name = 'PatchNC' #column name for the normal control data\n",
        "    NC_data = dataMat[NC_name]\n",
        "    num_NC =  NC_data.shape[0]\n",
        "    X_size,Y_size,Z_size = NC_data.shape[1:]\n",
        "    del dataMat\n",
        "    numAllSample = num_AD+num_NC\n",
        "    adLabel =  np.ones((num_AD,))\n",
        "    normalLabel =  np.zeros((num_NC,))\n",
        "    imgADNC_3D = np.zeros((numAllSample,49,39,38))\n",
        "    imgADNC_3D[:num_AD,:X_size,:Y_size,:Z_size] = AD_data[:,:49,:39,:38]\n",
        "    imgADNC_3D[num_AD:,:X_size,:Y_size,:Z_size] = NC_data[:,:49,:39,:38]\n",
        "    imgVector = imgADNC_3D.reshape(numAllSample,-1)\n",
        "    imgVector = imgVector.astype('float16')\n",
        "    imgVector = (imgVector-np.min(imgVector))*2.0/(float(np.max(imgVector)-np.min(imgVector)))-1\n",
        "    data3dvt = imgVector\n",
        "    imgLabel = np.hstack((adLabel,normalLabel))\n",
        "    return data3dvt,imgLabel "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0d7uDAQg47gh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_cv(cv_name = 'index_10fold.pkl',fold_idx = 0):\n",
        "    #taking in a pickled file which contains \n",
        "    input_doc = open(cv_name,'rb') #read binary data as is without any transformations such as converting newlines to/from platform-specific values \n",
        "                                   #or decoding/encoding text using a character encoding\n",
        "    in_data = pickle.load(input_doc) #we should be writing our binary file for the pickle in such a format as \n",
        "                                     #favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
        "                                     #pickle.dump(favorite_color, open( \"save.p\", \"wb\" ))\n",
        "    input_doc.close() \n",
        "    train_idx,test_idx = in_data #from the pickle file, we should be getting a tuple from each pickled file which contains two arrays, each of size ten, \n",
        "                                 #and iterates through each entry in both of them\n",
        "    train_index0 = train_idx[fold_idx] #gets the entry of the training array at the specified fold index\n",
        "    test_index = test_idx[fold_idx] #gets the entry of the testing array at the specified fold index\n",
        "    val_index = test_idx[fold_idx-1] #gets a validation entry from the testing array at the specified fold index minus 1\n",
        "    train_index = list(set(train_index0)-set(val_index))\n",
        "    train_index = np.array(train_index)\n",
        "    return train_index,test_index,val_index "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-l5faEQ47gk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def svc(traindata,trainlabel,testdata,testlabel):\n",
        "    print(\"Start training SVM...\")\n",
        "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
        "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
        "    clf = grid_search.GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
        "    clf = clf.fit(traindata,trainlabel)\n",
        "    print(\"Best estimator found by grid search:\")\n",
        "    print(clf.best_estimator_)\n",
        "    pred_testlabel = clf.predict(testdata)\n",
        "    return pred_testlabel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2jQZzY147gm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_train0_test1_val2(aug_time,train0test1val2,fold_idx,image_idx,patch_idx):\n",
        "    train_id,test_id,val_id = load_cv(fold_idx = fold_idx)\n",
        "\n",
        "    if (train0test1val2 == 0):\n",
        "        return_index = train_id\n",
        "    elif (train0test1val2 == 1):\n",
        "        return_index = test_id\n",
        "    else :\n",
        "        return_index = val_id\n",
        "    if (aug_time==1):\n",
        "        dataVct,image_label = load_data(patch_idx,image_idx) \n",
        "        dataVct = dataVct[return_index]\n",
        "        data_label = image_label[return_index]\n",
        "    else:\n",
        "        for i in range(aug_time):\n",
        "            if (i==0):\n",
        "                dataVct,image_label = load_data(patch_idx,i) \n",
        "                dataVct = dataVct[return_index]\n",
        "                data_label = image_label[return_index]\n",
        "            else:\n",
        "                Vct,image_label = load_data(patch_idx,i)\n",
        "                Vct = Vct[return_index]\n",
        "                dataVct = np.concatenate((dataVct,Vct),axis = 0)\n",
        "                img_label = image_label[return_index]\n",
        "                data_label = np.hstack((data_label,img_label))\n",
        "    np.random.seed(1234)   \n",
        "    random_idx = np.random.permutation(range(len(data_label)))            \n",
        "    dataX = dataVct[random_idx]\n",
        "    dataY = data_label[random_idx]\n",
        "    length_v = data_label.shape[0]        \n",
        "    dataX = dataX.reshape(length_v, 1,49,39,38)\n",
        "    if train0test1val2==1:\n",
        "        randomIndex = return_index[random_idx]\n",
        "    else:\n",
        "        randomIndex = random_idx\n",
        "\n",
        "    return dataX,dataY,randomIndex\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ALr2XDz47go",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(model_para = [6, 12, 18, 18, 64, 0.01, 0.4,(5, 5, 5)],foldname = os.path.abspath('.') ):\n",
        "    # input image dimensions\n",
        "    img_rows, img_cols, img_depth = 49,39,38\n",
        "    \n",
        "    # number of convolutional filters to use\n",
        "    conv_l2 = 0.008\n",
        "    full_l2  = 0.3\n",
        "    \n",
        "    # convolution kernel size\n",
        "    kernel_size = (3,3,3)\n",
        "    \n",
        "    # size of pooling area for max pooling\n",
        "    pool_size = (2, 2, 2)\n",
        "\n",
        "    drop_out = (model_para[5], model_para[6])\n",
        "    \n",
        "    act_function = 'tanh'\n",
        "    \n",
        "    full_connect = model_para[4]\n",
        "    \n",
        "    nb_filters = (model_para[0], model_para[1], model_para[2], model_para[3])\n",
        "#    nb_filters = (5, 10, 15, 15)\n",
        "\n",
        "    l1_regularizer = 0.01\n",
        "    \n",
        "    l2_regularizer = full_l2\n",
        "    \n",
        "    nb_classes = 2\n",
        "    \n",
        "    input_shape = (1, img_rows, img_cols, img_depth)\n",
        "    \n",
        "    wr = Regularizer(l1=l1_regularizer,l2=l2_regularizer)\n",
        "    #creat cnn model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Convolution3D(nb_filters[0], kernel_size[0], kernel_size[1],kernel_size[2],W_regularizer = l2(conv_l2),\n",
        "                            activation = act_function , input_shape=input_shape))                  \n",
        "    model.add(MaxPooling3D(pool_size=pool_size)) \n",
        "\n",
        "    model.add(Dropout(drop_out[0]))\n",
        "    \n",
        "    model.add(Convolution3D(nb_filters[1], kernel_size[0], kernel_size[1], kernel_size[2],W_regularizer = l2(conv_l2),\n",
        "                               activation = act_function))\n",
        "    model.add(MaxPooling3D(pool_size=pool_size))\n",
        "    \n",
        "    model.add(Dropout(drop_out[0]))\n",
        "    \n",
        "    model.add(Convolution3D(nb_filters[2], kernel_size[0], kernel_size[1], kernel_size[2],W_regularizer = l2(conv_l2),\n",
        "                                activation = act_function))                    \n",
        "    model.add(MaxPooling3D(pool_size=pool_size))    \n",
        "    \n",
        "    model.add(Dropout(drop_out[0]))\n",
        "\n",
        "    model.add(Convolution3D(nb_filters[3], kernel_size[0], kernel_size[1], kernel_size[2],W_regularizer = l2(conv_l2),\n",
        "                            activation = act_function))\n",
        "#    model.add(MaxPooling3D(pool_size=pool_size))    \n",
        "    \n",
        "    model.add(Dropout(drop_out[1]/2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(full_connect, W_regularizer = wr,activation = act_function))\n",
        "\n",
        "    model.add(Dropout(drop_out[1]))\n",
        "    \n",
        "    model.add(Dense(nb_classes,activation = act_function))\n",
        "    \n",
        "    model.add(Activation('softmax'))\n",
        "#    model.add(Activation(act_function))\n",
        "    model.summary()\n",
        "    \n",
        "    ADA = Adadelta(lr = 2.0, rho=0.95)\n",
        "    \n",
        "    model.compile(loss= 'categorical_crossentropy',\n",
        "              optimizer= ADA,\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "##    save parameters of cnn model to .txt             \n",
        "    sname = 'model_parameter.txt'\n",
        "    full_namem = os.path.join(foldname,sname)\n",
        "    fm = open(full_namem,'wb')\n",
        "    fm.write('************CNN model parameter************ '+'\\n')\n",
        "    fm.write('Number of Convolution layer :     '+str(len(nb_filters))+'\\n')\n",
        "    fm.write('Input shape :                     '+str(input_shape)+'\\n')\n",
        "    fm.write('Number of kernal per layer ï¼?    '+str(nb_filters)+'\\n')\n",
        "    fm.write('Kernel size per layer :           '+str(kernel_size)+'\\n')\n",
        "    fm.write('Pool size per layer :             '+str(pool_size)+'\\n')\n",
        "    fm.write('Activation function per layer :   '+act_function+'\\n')\n",
        "#    fm.write('Dropout rate :                    '+str(drop_out)+'\\n')\n",
        "    fm.write('Number of full-connect layer :    '+str(full_connect)+'\\n')\n",
        "    fm.write('Coefficient of L1 regularizer :   '+str(l1_regularizer)+'\\n')\n",
        "    fm.write('Coefficient of L2 regularizer :   '+str(l2_regularizer)+'\\n')\n",
        "    fm.write('Output :                          '+str(nb_classes)+' classes'+'\\n')\n",
        "    fm.close()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFZyz_pT47gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97cd8311-62a6-40d9-cd4f-e37fc941dfdc"
      },
      "cell_type": "code",
      "source": [
        "#start time for a timer for this chunk of the code, is this necessary?\n",
        "start_time = timeit.default_timer()\n",
        "nb_classes = 2 #number of classes used for converting the array of labeled data(from 0 to nb_classes-1) to one-hot vector using np.utils.to_categorical\n",
        "n_fold = 10 #number of folds for our cross validation\n",
        "curpath = os.path.abspath('.') #returns a normalized version of the current path '.'\n",
        "print(curpath)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h36DWu0A47gs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Dimensions of patches\n",
        "img_rows, img_cols, img_depth = 49,39,38  \n",
        "\n",
        "nb_epoch = 80\n",
        "bsize = 64           \n",
        "augtime = 8\n",
        "test_score = ['']*augtime\n",
        "Ytest_prd = ['']*augtime\n",
        "\n",
        "#creating 7 arrays of zeros with dimensions of 27 vectors and 10 columns each\n",
        "#finding the scores of accuracy for each of the 10 folds for each of the 27 patches\n",
        "testpara = 27\n",
        "record_train_acc = np.zeros((testpara,n_fold))\n",
        "record_test_acc = np.zeros((testpara,n_fold))\n",
        "record_val_acc = np.zeros((testpara,n_fold))\n",
        "record_train_loss = np.zeros((testpara,n_fold))\n",
        "record_test_loss = np.zeros((testpara,n_fold))\n",
        "record_val_loss = np.zeros((testpara,n_fold))\n",
        "record_svm_acc = np.zeros((testpara,n_fold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wUuTNEJ47gu",
        "colab_type": "code",
        "outputId": "5449edc0-a92c-4cd3-8594-92558f9bd871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "cell_type": "code",
      "source": [
        "#for each of the 27 patches\n",
        "for rii in range(27):\n",
        "    t1_time = timeit.default_timer() #getting time for another timer\n",
        "    filtersize = (3,3,3)\n",
        "    pri = rii\n",
        "    numi = 0\n",
        "    saveSC = np.zeros((n_fold,2))\n",
        "\n",
        "    saveYP=['']*n_fold\n",
        "    saveYP_svm =['']*n_fold\n",
        "    saveSC_svm =np.zeros((n_fold,1))\n",
        "    title = os.path.split(__file__)[1][:-3] \n",
        "    sub_title = 'Patch_'+str(rii)\n",
        "    new_fold0 = os.path.join(curpath,title)\n",
        "    new_fold = os.path.join(new_fold0,sub_title)\n",
        "    if not os.path.isdir(new_fold):\n",
        "        os.makedirs(new_fold)\n",
        "    name_para = 'train_para_'+str(rii)+'.txt'\n",
        "    full_namep = os.path.join(new_fold,name_para)\n",
        "    fp = open(full_namep,'wb')\n",
        "    fp.write('************  CNN training parameter  ************ '+'\\n')\n",
        "    fp.write('Fold number:         '+str(n_fold)+'\\n')\n",
        "    fp.write('Number of epoch:     '+str(nb_epoch)+'\\n')\n",
        "    fp.write('Batch size:          '+str(bsize)+'\\n')\n",
        "    fp.write('Training Targe:      '+str(nb_classes)+' classes'+'\\n')\n",
        "    fp.close()\n",
        "    \n",
        "    for fold_i in range(n_fold):\n",
        "        np.random.seed(1234)\n",
        "        print('========== Now running on fold '+ str(fold_i+1) + ' ==========')\n",
        "        X_train,Y_train,rt = data_train0_test1_val2(aug_time = augtime,train0test1val2 = 0,fold_idx = fold_i,image_idx = 0,patch_idx = rii+1)\n",
        "        X_test,Y_test,rdxIdx = data_train0_test1_val2(aug_time = 1,train0test1val2 = 1,fold_idx = fold_i,image_idx = 0,patch_idx = rii+1)\n",
        "        X_val,Y_val,rv = data_train0_test1_val2(aug_time = augtime,train0test1val2 = 2,fold_idx = fold_i,image_idx = 0,patch_idx = rii+1)\n",
        "        \n",
        "        Y_train = np.array(Y_train, dtype='float32')\n",
        "        Y_val = np.array(Y_val, dtype='float32')\n",
        "        Y_train_n = np_utils.to_categorical(Y_train, nb_classes)\n",
        "        Y_test_n = np_utils.to_categorical(Y_test, nb_classes)\n",
        "        Y_val_n = np_utils.to_categorical(Y_val, nb_classes)\n",
        "        \n",
        "        model = create_model(model_para = [15,25,50,50,40, 0.10, 0.6,(3, 3, 3)], foldname = new_fold)\n",
        "    \n",
        "        name_wts = 'weights_'+str(fold_i+1)+'.hdf5'\n",
        "        name_log = 'training_'+str(fold_i+1)+'.log'\n",
        "        traininglog = os.path.join(new_fold,name_log)\n",
        "        best_weight_name = os.path.join(new_fold,name_wts)\n",
        "        csv_logger = CSVLogger(traininglog)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                  patience=3, min_lr=0.0001)\n",
        "        checkpointer = ModelCheckpoint(filepath=best_weight_name, verbose=1, save_best_only=True)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "        logif = model.fit(X_train, Y_train_n, batch_size=bsize, nb_epoch=nb_epoch, shuffle = False,\n",
        "           verbose=1,validation_data=(X_val,Y_val_n),callbacks = [csv_logger,reduce_lr,checkpointer,early_stopping])\n",
        "        del X_train\n",
        "        \n",
        "        loss_history = logif.history\n",
        "        name_log = 'loss_history_'+str(fold_i+1)+'.txt'\n",
        "        full_log_name = os.path.join(new_fold,name_log)\n",
        "        logf = open(full_log_name,'w')\n",
        "        \n",
        "        for keyi in loss_history:\n",
        "            write_str = keyi + ': ' + str(loss_history[keyi]) + '\\n'\n",
        "            logf.write(write_str)  \n",
        "        logf.close()\n",
        "        \n",
        "        model.load_weights(best_weight_name)\n",
        "        test_score = model.evaluate(X_test, Y_test_n,batch_size=bsize, verbose=1)\n",
        "        Ytest_prd = model.predict(X_test,batch_size=bsize)\n",
        "        print('Test score' +':', test_score[0])\n",
        "        print('Test accuracy' +':', test_score[1])\n",
        "        \n",
        "#        record_test_acc_all[pri,fold_i] = test_score[1]\n",
        "#        record_test_loss_all[pri,fold_i] = test_score[0]\n",
        "        X_val,Y_val,rv = data_train0_test1_val2(aug_time = 1,train0test1val2 = 2,fold_idx = fold_i,image_idx = 0,patch_idx = rii+1)\n",
        "        Y_val_n = np_utils.to_categorical(Y_val, nb_classes)\n",
        "\tval_score = model.evaluate(X_val, Y_val_n,batch_size=bsize, verbose=1)\n",
        "        val_prd = model.predict(X_val,batch_size=bsize)\n",
        "\n",
        "        X_train,Y_train,train_rdnidx = data_train0_test1_val2(aug_time = 1,train0test1val2 = 0,fold_idx = fold_i,image_idx = 0,patch_idx = rii+1)\n",
        "        Y_train = np.array(Y_train, dtype='float32')\n",
        "        Y_train_n = np_utils.to_categorical(Y_train, nb_classes)\n",
        "        train_score = model.evaluate(X_train, Y_train_n,batch_size=bsize, verbose=1)\n",
        "        Ytrain_prd = model.predict(X_train,batch_size=bsize)\n",
        "        \n",
        "        print('Train score:', train_score[0])\n",
        "        print('Train accuracy:', train_score[1])\n",
        "        record_train_acc[pri,fold_i] = train_score[1]\n",
        "        record_train_loss[pri,fold_i] = train_score[0]\n",
        "        \n",
        "        print('Val score:', val_score[0])\n",
        "        print('Val accuracy:', val_score[1])\n",
        "        record_val_acc[pri,fold_i] = val_score[1]\n",
        "        record_val_loss[pri,fold_i] = val_score[0]\n",
        "        \n",
        "        print('Test score:', test_score[0])\n",
        "        print('Test accuracy:', test_score[1])\n",
        "        record_test_acc[pri,fold_i] = test_score[1]\n",
        "        record_test_loss[pri,fold_i] = test_score[0]\n",
        "        saveSC[fold_i,:]=test_score[0] \n",
        "        saveYP[fold_i]=Ytest_prd[0]\n",
        "\n",
        "        json_string = model.to_json()\n",
        "        name_json = 'model_architecture_'+str(fold_i+1)+'.json'\n",
        "        full_namej = os.path.join(new_fold,name_json)\n",
        "        open(full_namej,'w').write(json_string)  \n",
        "        name_model = 'model_weights_'+str(fold_i+1)+'.h5'\n",
        "        full_namem = os.path.join(new_fold,name_model)\n",
        "        model.save_weights(full_namem)\n",
        "##        define theano funtion to get output of FC layer\n",
        "        get_feature = K.function([model.layers[0].input,K.learning_phase()],[model.layers[12].output])\n",
        "        get_feature1 = K.function([model.layers[0].input,K.learning_phase()],[model.layers[9].output])\n",
        "        get_feature2 = K.function([model.layers[0].input,K.learning_phase()],[model.layers[7].output])      \n",
        "#        get_feature3 = K.function([model.layers[0].input,K.learning_phase()],[model.layers[5].output])\n",
        "        FC_train_feature = np.zeros(())\n",
        "        FC_train_feature1 = np.zeros(())\n",
        "        FC_train_feature2 = np.zeros(())\n",
        "#        FC_train_feature3 = np.zeros(())\n",
        "        train_num = X_train.shape[0]\n",
        "        X_train_d = ['']*3\n",
        "        Y_train_d = ['']*3\n",
        "        X_train_d[0] = X_train[:train_num//3]\n",
        "        Y_train_d[0] = Y_train[:train_num//3]\n",
        "        X_train_d[1] = X_train[train_num//3:train_num//3*2]\n",
        "        Y_train_d[1] = Y_train[train_num//3:train_num//3*2]\n",
        "        X_train_d[2] = X_train[train_num//3*2:]\n",
        "        Y_train_d[2] = Y_train[train_num//3*2:]\n",
        "        for i in range(3):\n",
        "            if (i == 0):\n",
        "                FC_train_feature = get_feature([X_train_d[0],1])[0]\n",
        "                FC_train_feature1 = get_feature1([X_train_d[0],1])[0]\n",
        "                FC_train_feature2 = get_feature2([X_train_d[0],1])[0]\n",
        "#                FC_train_feature3 = get_feature3([X_train_d[0],1])[0]\n",
        "                Y_train_new = Y_train_d[0]\n",
        "            else:\n",
        "                FC_train_feature = np.concatenate((FC_train_feature,get_feature([X_train_d[i],1])[0]),axis = 0) \n",
        "                Y_train_new = np.concatenate((Y_train_new,Y_train_d[i]),axis = 0)\n",
        "                FC_train_feature1 = np.concatenate((FC_train_feature1,get_feature1([X_train_d[i],1])[0]),axis = 0)\n",
        "                FC_train_feature2 = np.concatenate((FC_train_feature2,get_feature2([X_train_d[i],1])[0]),axis = 0)\n",
        "#                FC_train_feature3 = np.concatenate((FC_train_feature3,get_feature3([X_train_d[i],1])[0]),axis = 0)\n",
        "        FC_test_feature = get_feature([X_test,0])[0]\n",
        "        FC_test_feature1 = get_feature1([X_test,0])[0]\n",
        "        FC_test_feature2 = get_feature2([X_test,0])[0]\n",
        "#        FC_test_feature3 = get_feature3([X_test,0])[0]\n",
        "        Y_test_new  = Y_test\n",
        "        FC_val_feature = get_feature([X_val,0])[0]\n",
        "        FC_val_feature1 = get_feature1([X_val,0])[0]\n",
        "        FC_val_feature2 = get_feature2([X_val,0])[0]\n",
        "#        FC_val_feature3 = get_feature3([X_val,0])[0]\n",
        "        Y_val_new  = Y_val\n",
        "        name_feat = 'model_fold_'+str(fold_i+1)+'.mat'\n",
        "        name_feat1 = 'model_flatten_l1_'+str(fold_i+1)+'.mat'\n",
        "        name_feat2 = 'model_flatten_l2_'+str(fold_i+1)+'.mat'  \n",
        "#        name_feat3 = 'model_flatten_l3_'+str(fold_i+1)+'.mat'\n",
        "        full_feat_mat = os.path.join(new_fold,name_feat)\n",
        "        full_feat_mat1 = os.path.join(new_fold,name_feat1)\n",
        "        full_feat_mat2 = os.path.join(new_fold,name_feat2)        \n",
        "#        full_feat_mat3 = os.path.join(new_fold,name_feat3)       \n",
        "        sio.savemat(full_feat_mat,{'train_feature':FC_train_feature,'train_label':Y_train_new,\n",
        "                                   'val_feature':FC_val_feature,'val_label':Y_val_new,\n",
        "                                   'test_feature':FC_test_feature,'test_label':Y_test_new})\n",
        "        sio.savemat(full_feat_mat1,{'train_feature':FC_train_feature1,'train_label':Y_train_new,\n",
        "                                   'val_feature':FC_val_feature1,'val_label':Y_val_new,\n",
        "                                   'test_feature':FC_test_feature1,'test_label':Y_test_new})\n",
        "        sio.savemat(full_feat_mat2,{'train_feature':FC_train_feature2,'train_label':Y_train_new,\n",
        "                                   'val_feature':FC_val_feature2,'val_label':Y_val_new,\n",
        "                                   'test_feature':FC_test_feature2,'test_label':Y_test_new})\n",
        "#        sio.savemat(full_feat_mat3,{'train_feature':FC_train_feature3,'train_label':Y_train_new,\n",
        "#                                   'val_feature':FC_val_feature3,'val_label':Y_val_new,\n",
        "#                                   'test_feature':FC_test_feature3,'test_label':Y_test_new})\n",
        "        \n",
        "        pred_label = svc(FC_train_feature,Y_train_new,FC_test_feature,Y_test_new)\n",
        "        svm_acc = metrics.accuracy_score(pred_label,Y_test_new)\n",
        "        svm_recall = metrics.recall_score(pred_label,Y_test_new)\n",
        "        svm_precision = metrics.precision_score(pred_label,Y_test_new)\n",
        "        print('CNN - SVM accuracy score:', svm_acc)\n",
        "        print('CNN - SVM recall score:', svm_recall)\n",
        "        print('CNN - SVM precision score:', svm_precision)\n",
        "#        saveSC_svm[fold_i,:]=svm_acc \n",
        "        record_svm_acc[pri,fold_i] = svm_acc \n",
        "        saveYP_svm[fold_i]=pred_label\n",
        "        if fold_i==0:\n",
        "            test_pro = Ytest_prd[:,1]\n",
        "#            test_pro = test_p.reshape((test_p.shape[0],1))\n",
        "            rd_test = rdxIdx\n",
        "            true_test = Y_test\n",
        "        else:\n",
        "            test_p = Ytest_prd[:,1]\n",
        "#            test_p = test_p.reshape((test_p.shape[0],1))\n",
        "            test_pro = np.concatenate((test_pro,test_p),axis =0)\n",
        "            rd_test = np.concatenate((rd_test,rdxIdx),axis=0)\n",
        "            true_test = np.concatenate((true_test,Y_test),axis=0)\n",
        "#        numi = numi+1\n",
        "#    name_sv = 'result'+'.pkl'\n",
        "    name_sv = 'result'+'.mat'\n",
        "    full_namesv = os.path.join(new_fold,name_sv)\n",
        "    sio.savemat(full_namesv,{'test_prob':test_pro,'random_test_index':rd_test,'true_label':true_test})\n",
        "#    fr = open(full_namesv,'wb')        \n",
        "#    outs = [saveYP,saveSC,saveSC_svm,saveYP_svm]\n",
        "#    pickle.dump(outs,fr)\n",
        "#    fr.close()\n",
        "    name_rs = 'result_'+'run_para' +str(pri)+'.txt'\n",
        "    full_namers = os.path.join(new_fold,name_rs)\n",
        "    ft = open(full_namers,'wb')\n",
        "    ft.write('*************  Result of CNN model  ************ '+'\\n'+'\\n')\n",
        "    ft.write('Loss and accuracy :'+ '\\n')\n",
        "    for idl in range(n_fold):\n",
        "        ft.write('Fold  '+str(idl+1) +'\\n')\n",
        "        ft.write('   Test  :'+ str(record_test_acc[pri,idl]) +'   '+ str(record_test_loss[pri,idl]) +'\\n')\n",
        "        ft.write('   Train :'+ str(record_train_acc[pri,idl]) +'   '+ str(record_train_loss[pri,idl]) +'\\n')\n",
        "        ft.write('   Val   :'+ str(record_val_acc[pri,idl]) +'   '+ str(record_val_loss[pri,idl]) +'\\n')\n",
        "    dacc = np.mean(record_test_acc[pri,:])\n",
        "    avg_loss = np.mean(record_test_loss[pri,:])\n",
        "    ft.write('Test accuracy for whole data:   %f %%' %(dacc*100.)+'\\n')\n",
        "    ft.write('The average loss:    '+ str(avg_loss)+'\\n'+'\\n')\n",
        "    \n",
        "    ft.write('************  Test result of SVM from CNN feature  ************ '+'\\n'+'\\n')\n",
        "    ft.write('Test accuracy :'+ '\\n')\n",
        "    for idl in range(n_fold):\n",
        "        ft.write('Fold '+str(idl+1)+ ':   '+str(record_svm_acc[pri,idl]) +'\\n')\n",
        "    dacc_svm = np.mean(record_svm_acc[pri,:])\n",
        "    ft.write('Test accuracy for whole data:   %f %%' %(dacc_svm*100.)+'\\n')\n",
        "    t2_time = timeit.default_timer()\n",
        "    ft.write(('The code for this parameter ran for %.2fm' % ((t2_time -t1_time) / 60.)))\n",
        "    ft.close()\n",
        "\n",
        "    end_time = timeit.default_timer()\n",
        "    finalresult_name = os.path.join(new_fold0,'final_result.mat')\n",
        "#    finalresult_name_aug = os.path.join(new_fold0,'final_result_for_aug.mat')\n",
        "    avg_train_acc = np.mean(record_train_acc,axis = 1)\n",
        "    avg_train_acc = avg_train_acc.T\n",
        "    avg_test_acc = np.mean(record_test_acc,axis = 1)\n",
        "    avg_test_acc = avg_test_acc.T\n",
        "    avg_val_acc = np.mean(record_val_acc,axis = 1)\n",
        "    avg_val_acc = avg_val_acc.T\n",
        "    avg_train_loss = np.mean(record_train_loss,axis = 1)\n",
        "    avg_test_loss = np.mean(record_test_loss,axis = 1)\n",
        "    avg_val_loss = np.mean(record_val_loss,axis = 1)\n",
        "    sio.savemat(finalresult_name,{'train_acc':record_train_acc,'test_acc':record_test_acc,'val_acc':record_val_acc,\n",
        "                                'train_loss':record_train_loss,'test_loss':record_test_loss,'val_loss':record_val_loss,\n",
        "                                'avg_train_acc':avg_train_acc,'avg_test_acc':avg_test_acc,'avg_val_acc':avg_val_acc,\n",
        "                                'avg_train_loss':avg_train_loss,'avg_test_loss':avg_test_loss,'avg_val_loss':avg_val_loss})\n",
        "#    sio.savemat(finalresult_name_aug,{'test_acc_all':record_test_acc_all,'test_loss_all':record_test_loss_all})\n",
        "    \n",
        "    print(('The code for file ' +\n",
        "    os.path.split(__file__)[1] +\n",
        "        ' ran for %.2fm' % ((end_time - start_time) / 60.)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f83151bc4d41>\"\u001b[0;36m, line \u001b[0;32m79\u001b[0m\n\u001b[0;31m    val_score = model.evaluate(X_val, Y_val_n,batch_size=bsize, verbose=1)\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IEMLNeeY47gy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('final_result.mat',{'train_acc':record_train_acc,'test_acc':record_test_acc,'val_acc':record_val_acc,\n",
        "                                'train_loss':record_train_loss,'test_loss':record_test_loss,'val_loss':record_val_loss,\n",
        "                                'avg_train_acc':avg_train_acc,'avg_test_acc':avg_test_acc,'avg_val_acc':avg_val_acc,\n",
        "                                'avg_train_loss':avg_train_loss,'avg_test_loss':avg_test_loss,'avg_val_loss':avg_val_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}